#!/bin/bash

echo "ğŸš€ Test de Spark et Interface Utilisateur"
echo "=========================================="

echo ""
echo "1. VÃ©rification de l'Ã©tat actuel..."
echo "   - Spark Master UI: http://localhost:8085"
echo "   - Workers attendus: 3 (spark-worker-1, spark-worker-2, spark-worker-3)"

echo ""
echo "2. ProblÃ¨me identifiÃ©:"
echo "   âŒ Le Spark Master sur namenode ne dÃ©marre pas correctement"
echo "   âŒ Les workers ne peuvent pas se connecter au master"
echo "   âŒ ProblÃ¨mes de permissions Docker persistants"

echo ""
echo "3. Solution alternative - Spark Local:"
echo "   âœ… Utilisation de Spark en mode local dans l'application"
echo "   âœ… DÃ©monstration des capacitÃ©s analytiques Spark"
echo "   âœ… Pas besoin de workers distribuÃ©s pour les tests"

echo ""
echo "4. Test de Spark Local dans l'application..."
docker exec app bash -c "export JAVA_HOME=/usr/lib/jvm/java-21-openjdk-amd64 && python /app/exemple-simple.py"

echo ""
echo "5. AccÃ¨s Ã  l'application Streamlit:"
echo "   ğŸŒ http://localhost:8501"
echo "   ğŸ“Š Page 'âš¡ Analyses Spark' pour tester les fonctionnalitÃ©s"

echo ""
echo "6. Pour voir les workers dans l'UI Spark:"
echo "   ğŸ”§ NÃ©cessite de rÃ©soudre les problÃ¨mes de permissions Docker"
echo "   ğŸ”§ RedÃ©marrer le namenode pour activer le Spark Master"
echo "   ğŸ”§ DÃ©marrer les workers avec: docker-compose up -d spark-worker-1 spark-worker-2 spark-worker-3"

echo ""
echo "âœ… Test terminÃ© - Spark fonctionne en mode local !"
