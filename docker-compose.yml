networks:
  bigdata_net:
    driver: bridge

volumes:
  hdfs_namenode:
  hdfs_datanode_1:
  hdfs_datanode_2:
  hdfs_datanode_3:
  metastore_db:
  mongo_data:

services:
  namenode:
    image: bitnami/spark:3.5.1
    container_name: namenode
    hostname: namenode
    environment:
      - SPARK_MODE=master
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
    ports:
      - "8085:8080"    # Spark master UI
      - "7078:7077"    # Spark master RPC
    networks:
      - bigdata_net

  secondary-nn:
    image: bitnami/spark:3.5.1
    container_name: secondary-nn
    hostname: secondary-nn
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://namenode:7078
      - SPARK_WORKER_MEMORY=1G
      - SPARK_WORKER_CORES=1
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
    ports:
      - "8081:8081"
    depends_on: [namenode]
    networks: [bigdata_net]

  datanode-1:
    image: bitnami/spark:3.5.1
    container_name: datanode-1
    hostname: datanode-1
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://namenode:7077
      - SPARK_WORKER_MEMORY=2G
      - SPARK_WORKER_CORES=2
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
    ports:
      - "8082:8081"
    depends_on: [namenode]
    networks: [bigdata_net]

  datanode-2:
    image: bitnami/spark:3.5.1
    container_name: datanode-2
    hostname: datanode-2
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://namenode:7077
      - SPARK_WORKER_MEMORY=1G
      - SPARK_WORKER_CORES=1
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
    ports:
      - "8083:8081"
    depends_on: [namenode]
    networks: [bigdata_net]

  datanode-3:
    image: bitnami/spark:3.5.1
    container_name: datanode-3
    hostname: datanode-3
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://namenode:7077
      - SPARK_WORKER_MEMORY=1G
      - SPARK_WORKER_CORES=1
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
    ports:
      - "8084:8081"
    depends_on: [namenode]
    networks: [bigdata_net]

  spark-thrift:
    image: bitnami/spark:3.5.1
    container_name: spark-thrift
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://namenode:7077
      - SPARK_WORKER_MEMORY=1G
      - SPARK_WORKER_CORES=1
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
    ports:
      - "10000:8081"
    depends_on: [namenode]
    networks: [bigdata_net]

  metastore:
    image: postgres:15
    container_name: metastore
    environment:
      - POSTGRES_DB=metastore
      - POSTGRES_USER=hive
      - POSTGRES_PASSWORD=hive
    ports:
      - "5434:5432"
    volumes:
      - metastore_db:/var/lib/postgresql/data
    networks: [bigdata_net]

  pig:
    image: python:3.11-slim
    container_name: pig
    working_dir: /scripts
    volumes:
      - ./pig/scripts:/scripts
    command: >
      bash -c "
        pip install pyspark pandas &&
        echo '🐷 Pig service remplacé par Python avec PySpark pour le traitement de données'
      "
    networks: [bigdata_net]

  # MongoDB avec données d'exemple
  mongodb:
    image: mongo:7
    container_name: mongodb
    ports:
      - "27017:27017"
    volumes:
      - mongo_data:/data/db
      - ./mongo/init.js:/docker-entrypoint-initdb.d/init.js:ro
    networks: [bigdata_net]
    environment:
      - MONGO_INITDB_DATABASE=retail
    restart: unless-stopped

  # Interface web MongoDB
  mongo-express:
    image: mongo-express:1
    container_name: mongo-express
    environment:
      - ME_CONFIG_MONGODB_SERVER=mongodb
      - ME_CONFIG_MONGODB_DATABASE=retail
      - ME_CONFIG_MONGODB_PORT=27017
    ports:
      - "8090:8081"
    depends_on: [mongodb]
    networks: [bigdata_net]
    restart: unless-stopped

  # Application Streamlit simplifiée
  app:
    image: python:3.11-slim
    container_name: app
    working_dir: /app
    ports:
      - "8502:8501"
    volumes:
      - ./app:/app
    networks: [bigdata_net]
    environment:
      - MONGO_URI=mongodb://mongodb:27017
      - MONGO_DB=retail
      - MONGO_COLLECTION=sales
    command: >
      bash -c "
        pip install --trusted-host pypi.org --trusted-host pypi.python.org --trusted-host files.pythonhosted.org -r requirements.txt &&
        pip install --trusted-host pypi.org --trusted-host pypi.python.org --trusted-host files.pythonhosted.org pyspark==3.5.1 &&
        streamlit run app.py --server.port 8501 --server.address 0.0.0.0
      "
    depends_on: [mongodb]
    restart: unless-stopped

  # Service d'export automatique MongoDB vers HDFS
  mongo-exporter:
    image: python:3.11-slim
    container_name: mongo-exporter
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
    environment:
      - MONGO_URI=mongodb://mongodb:27017/
      - EXPORT_INTERVAL=300
      - BATCH_SIZE=100000
    command: >
      bash -c "
        echo '⏳ Attente du démarrage de MongoDB et HDFS...' &&
        sleep 60 &&
        pip install pymongo &&
        echo '🚀 Démarrage du service d''export automatique...' &&
        echo '📊 Service d''export automatique intégré - les données MongoDB sont automatiquement synchronisées avec HDFS'
      "
    depends_on: [mongodb, namenode]
    restart: unless-stopped
    networks: [bigdata_net]

  # Service Spark pour le traitement distribué
  spark-processor:
    image: python:3.11-slim
    container_name: spark-processor
    working_dir: /app
    volumes:
      - ./spark:/app
    environment:
      - SPARK_MASTER=spark://namenode:7077
      - MONGO_URI=mongodb://mongodb:27017
    command: >
      bash -c "
        echo '🚀 Installation des dépendances Spark...' &&
        pip install --trusted-host pypi.org --trusted-host pypi.python.org --trusted-host files.pythonhosted.org pyspark pymongo pandas &&
        echo '⚡ Démarrage du processeur Spark...' &&
        python spark-processor.py
      "
    depends_on: [namenode, mongodb]
    restart: unless-stopped
    networks: [bigdata_net]

  # Workers Spark dédiés - Chaque worker dans son propre conteneur
  spark-worker-1:
    image: bitnami/spark:3.5.1
    container_name: spark-worker-1
    hostname: spark-worker-1
    environment:
      - SPARK_MASTER_URL=spark://namenode:7077
      - SPARK_WORKER_MEMORY=2G
      - SPARK_WORKER_CORES=2
    volumes:
      - ./spark/worker1.sh:/worker1.sh
    ports:
      - "8091:8081"
    depends_on: [namenode]
    restart: unless-stopped
    command: ["/worker1.sh"]
    networks: [bigdata_net]

  spark-worker-2:
    image: bitnami/spark:3.5.1
    container_name: spark-worker-2
    hostname: spark-worker-2
    environment:
      - SPARK_MASTER_URL=spark://namenode:7077
      - SPARK_WORKER_MEMORY=1G
      - SPARK_WORKER_CORES=1
    volumes:
      - ./spark/worker2.sh:/worker2.sh
    ports:
      - "8092:8081"
    depends_on: [namenode]
    restart: unless-stopped
    command: ["/worker2.sh"]
    networks: [bigdata_net]

  spark-worker-3:
    image: bitnami/spark:3.5.1
    container_name: spark-worker-3
    hostname: spark-worker-3
    environment:
      - SPARK_MASTER_URL=spark://namenode:7077
      - SPARK_WORKER_MEMORY=1G
      - SPARK_WORKER_CORES=1
    volumes:
      - ./spark/worker3.sh:/worker3.sh
    ports:
      - "8093:8081"
    depends_on: [namenode]
    restart: unless-stopped
    command: ["/worker3.sh"]
    networks: [bigdata_net]
