#!/bin/bash

# Script de d√©marrage pour l'architecture de traitement distribu√©
# Hadoop + Spark + Pig + MongoDB + Streamlit

set -e

echo "üöÄ D√©marrage de l'architecture de traitement distribu√©"
echo "=================================================="

# V√©rifier que Docker et Docker Compose sont install√©s
if ! command -v docker &> /dev/null; then
    echo "‚ùå Docker n'est pas install√©. Veuillez installer Docker."
    exit 1
fi

if ! command -v docker-compose &> /dev/null; then
    echo "‚ùå Docker Compose n'est pas install√©. Veuillez installer Docker Compose."
    exit 1
fi

# Cr√©er le fichier .env √† partir de env.example si il n'existe pas
echo "üìù Configuration des variables d'environnement..."
if [ ! -f ".env" ]; then
    echo "‚ö†Ô∏è  Fichier .env manquant. Cr√©ation √† partir de env.example..."
    cp env.example .env
    echo "‚úÖ Fichier .env cr√©√© avec succ√®s"
else
    echo "‚úÖ Fichier .env d√©j√† pr√©sent"
fi

# V√©rifier que les connecteurs sont pr√©sents
echo "üì¶ V√©rification des connecteurs..."
if [ ! -f "connectors/mongo-spark-connector_2.12-10.1.1.jar" ]; then
    echo "‚ö†Ô∏è  Connecteur MongoDB Spark manquant. T√©l√©chargement..."
    mkdir -p connectors
    cd connectors
    wget -q https://search.maven.org/remotecontent?filepath=org/mongodb/spark/mongo-spark-connector_2.12/10.1.1/mongo-spark-connector_2.12-10.1.1.jar -O mongo-spark-connector_2.12-10.1.1.jar
    cd ..
fi

if [ ! -f "connectors/postgresql-42.7.1.jar" ]; then
    echo "‚ö†Ô∏è  Driver PostgreSQL manquant. T√©l√©chargement..."
    mkdir -p connectors
    cd connectors
    wget -q https://jdbc.postgresql.org/download/postgresql-42.7.1.jar -O postgresql-42.7.1.jar
    cd ..
fi

# Construire les images Docker
echo "üî® Construction des images Docker..."
docker-compose build

# D√©marrer les services
echo "üöÄ D√©marrage des services..."
docker-compose up -d

# Attendre que les services soient pr√™ts
echo "‚è≥ Attente du d√©marrage des services..."
sleep 30

# V√©rifier l'√©tat des services
echo "üîç V√©rification de l'√©tat des services..."
docker-compose ps

# Attendre que HDFS soit pr√™t
echo "‚è≥ Attente que HDFS soit pr√™t..."
sleep 60

# V√©rifier HDFS
echo "üîç V√©rification de HDFS..."
docker exec namenode hdfs dfsadmin -report | head -20

# Lancer l'ingestion MongoDB vers HDFS
echo "üì• Lancement de l'ingestion MongoDB vers HDFS..."
docker exec spark-thrift python3 /opt/spark/scripts/mongo_to_hdfs.py

# Lancer le script Pig EDA
echo "üê∑ Lancement de l'analyse Pig..."
docker exec pig pig -x mapreduce /scripts/eda.pig

echo ""
echo "‚úÖ Architecture d√©marr√©e avec succ√®s!"
echo ""
echo "üìä Interfaces web disponibles:"
echo "  - HDFS NameNode: http://localhost:9870"
echo "  - YARN ResourceManager: http://localhost:8088"
echo "  - Spark Master: http://localhost:8080"
echo "  - Spark Worker 1: http://localhost:8081"
echo "  - Spark Worker 2: http://localhost:8082"
echo "  - Spark Worker 3: http://localhost:8083"
echo "  - MongoDB Express: http://localhost:8089"
echo "  - Application Streamlit: http://localhost:8501"
echo ""
echo "üîß Commandes utiles:"
echo "  - Voir les logs: docker-compose logs -f [service]"
echo "  - Arr√™ter: docker-compose down"
echo "  - Red√©marrer: docker-compose restart [service]"
echo ""
echo "üéØ Prochaines √©tapes:"
echo "  1. Ouvrir http://localhost:8501 pour acc√©der au dashboard"
echo "  2. V√©rifier les donn√©es dans MongoDB Express: http://localhost:8089"
echo "  3. Consulter les interfaces de monitoring Hadoop/Spark"
