#!/bin/bash

# Script de dÃ©marrage pour l'architecture de traitement distribuÃ©
# Hadoop + Spark + Pig + MongoDB + Streamlit

set -e

echo "ğŸš€ DÃ©marrage de l'architecture de traitement distribuÃ©"
echo "=================================================="

# VÃ©rifier que Docker et Docker Compose sont installÃ©s
if ! command -v docker &> /dev/null; then
    echo "âŒ Docker n'est pas installÃ©. Veuillez installer Docker."
    exit 1
fi

if ! command -v docker-compose &> /dev/null; then
    echo "âŒ Docker Compose n'est pas installÃ©. Veuillez installer Docker Compose."
    exit 1
fi

# CrÃ©er le fichier .env Ã  partir de env.example si il n'existe pas
echo "ğŸ“ Configuration des variables d'environnement..."
if [ ! -f ".env" ]; then
    echo "âš ï¸  Fichier .env manquant. CrÃ©ation Ã  partir de env.example..."
    cp env.example .env
    echo "âœ… Fichier .env crÃ©Ã© avec succÃ¨s"
else
    echo "âœ… Fichier .env dÃ©jÃ  prÃ©sent"
fi

# VÃ©rifier que les connecteurs sont prÃ©sents
echo "ğŸ“¦ VÃ©rification des connecteurs..."
if [ ! -f "connectors/mongo-spark-connector_2.12-10.1.1.jar" ]; then
    echo "âš ï¸  Connecteur MongoDB Spark manquant. TÃ©lÃ©chargement..."
    mkdir -p connectors
    cd connectors
    wget -q https://search.maven.org/remotecontent?filepath=org/mongodb/spark/mongo-spark-connector_2.12/10.1.1/mongo-spark-connector_2.12-10.1.1.jar -O mongo-spark-connector_2.12-10.1.1.jar
    cd ..
fi



# Construire les images Docker
echo "ğŸ”¨ Construction des images Docker..."
docker-compose build

# DÃ©marrer les services
echo "ğŸš€ DÃ©marrage des services..."
docker-compose up -d

# Attendre que les services soient prÃªts
echo "â³ Attente du dÃ©marrage des services..."
sleep 30

# VÃ©rifier l'Ã©tat des services
echo "ğŸ” VÃ©rification de l'Ã©tat des services..."
docker-compose ps

# Attendre que HDFS soit prÃªt
echo "â³ Attente que HDFS soit prÃªt..."
sleep 60

# VÃ©rifier HDFS
echo "ğŸ” VÃ©rification de HDFS..."
docker exec namenode hdfs dfsadmin -report | head -20

# VÃ©rifier que le service d'export automatique est dÃ©marrÃ©
echo "ğŸ“¥ VÃ©rification du service d'export automatique MongoDB vers HDFS..."
sleep 30

if docker ps | grep -q "mongo-exporter.*Up"; then
    echo "âœ… Service d'export automatique dÃ©marrÃ© avec succÃ¨s"
    echo "ğŸ”„ Les donnÃ©es MongoDB seront exportÃ©es automatiquement vers HDFS toutes les 5 minutes"
else
    echo "âš ï¸  Service d'export automatique non dÃ©tectÃ©, dÃ©marrage manuel..."
    docker-compose up -d mongo-exporter
    sleep 30
fi

# Lancer le script Pig EDA (optionnel - peut Ãªtre lancÃ© manuellement)
echo "ğŸ· Lancement de l'analyse Pig (optionnel)..."
docker exec pig pig -x mapreduce /scripts/eda.pig || echo "âš ï¸  Analyse Pig non disponible pour le moment"

echo ""
echo "âœ… Architecture dÃ©marrÃ©e avec succÃ¨s!"
echo ""
echo "ğŸ“Š Interfaces web disponibles:"
echo "  - HDFS NameNode: http://localhost:9870"
echo "  - YARN ResourceManager: http://localhost:8088"
echo "  - Spark Master: http://localhost:8080"
echo "  - Spark Worker 1: http://localhost:8081"
echo "  - Spark Worker 2: http://localhost:8082"
echo "  - Spark Worker 3: http://localhost:8083"
echo "  - MongoDB Express: http://localhost:8089"
echo "  - Application Streamlit: http://localhost:8501"
echo ""
echo "ğŸ”„ Services automatiques:"
echo "  - Export MongoDB â†’ HDFS: Automatique toutes les 5 minutes"
echo "  - Logs du service d'export: docker logs -f mongo-exporter"
echo ""
echo "ğŸ”§ Commandes utiles:"
echo "  - Voir les logs: docker-compose logs -f [service]"
echo "  - ArrÃªter: docker-compose down"
echo "  - RedÃ©marrer: docker-compose restart [service]"
echo ""
echo "ğŸ¯ Prochaines Ã©tapes:"
echo "  1. Ouvrir http://localhost:8501 pour accÃ©der au dashboard"
echo "  2. VÃ©rifier les donnÃ©es dans MongoDB Express: http://localhost:8089"
echo "  3. Consulter les interfaces de monitoring Hadoop/Spark"
