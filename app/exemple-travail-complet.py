#!/usr/bin/env python3
"""
Exemple de travail Spark complet pour d√©montrer le traitement distribu√©
"""

from pyspark.sql import SparkSession
from pyspark.sql.functions import *
import pymongo
import logging

# Configuration du logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def creer_session_spark():
    """Cr√©er une session Spark connect√©e au cluster"""
    return SparkSession.builder \
        .appName("TraitementDistribueVentes") \
        .config("spark.master", "spark://namenode:7077") \
        .config("spark.driver.memory", "1g") \
        .config("spark.executor.memory", "2g") \
        .config("spark.executor.cores", "2") \
        .config("spark.sql.adaptive.enabled", "true") \
        .config("spark.hadoop.fs.defaultFS", "hdfs://namenode:9000") \
        .enableHiveSupport() \
        .getOrCreate()

def charger_donnees_mongodb(spark, limite=50000):
    """Charger les donn√©es depuis MongoDB vers Spark"""
    logger.info(f"Chargement de {limite} documents depuis MongoDB...")
    
    client = pymongo.MongoClient("mongodb://mongodb:27017")
    db = client.retail
    collection = db.sales
    
    # Charger les donn√©es
    cursor = collection.find({}, {'_id': 0}).limit(limite)
    data = list(cursor)
    
    if not data:
        logger.error("Aucune donn√©e trouv√©e dans MongoDB")
        return None
    
    # Cr√©er le DataFrame Spark
    df = spark.createDataFrame(data)
    logger.info(f"‚úÖ {df.count()} lignes charg√©es dans Spark")
    
    return df

def analyser_ventes_par_categorie(df):
    """Analyser les ventes par cat√©gorie de produits"""
    logger.info("Analyse des ventes par cat√©gorie...")
    
    resultat = df.groupBy("category") \
        .agg(
            count("*").alias("nombre_ventes"),
            sum("total_amount").alias("revenu_total"),
            avg("total_amount").alias("montant_moyen"),
            max("total_amount").alias("vente_max"),
            sum("quantity").alias("quantite_totale")
        ) \
        .orderBy(col("revenu_total").desc())
    
    logger.info("‚úÖ Analyse par cat√©gorie termin√©e")
    return resultat

def analyser_top_produits(df, top_n=10):
    """Analyser les produits les plus vendus"""
    logger.info(f"Analyse des top {top_n} produits...")
    
    resultat = df.groupBy("product_name") \
        .agg(
            count("*").alias("nombre_ventes"),
            sum("quantity").alias("quantite_totale"),
            sum("total_amount").alias("revenu_total"),
            avg("total_amount").alias("prix_moyen")
        ) \
        .orderBy(col("revenu_total").desc()) \
        .limit(top_n)
    
    logger.info("‚úÖ Analyse des top produits termin√©e")
    return resultat

def analyser_utilisateurs_actifs(df, top_n=20):
    """Analyser les utilisateurs les plus actifs"""
    logger.info(f"Analyse des {top_n} utilisateurs les plus actifs...")
    
    resultat = df.groupBy("user_id") \
        .agg(
            count("*").alias("nombre_achats"),
            sum("total_amount").alias("montant_total"),
            avg("total_amount").alias("panier_moyen"),
            countDistinct("category").alias("categories_differentes")
        ) \
        .orderBy(col("montant_total").desc()) \
        .limit(top_n)
    
    logger.info("‚úÖ Analyse des utilisateurs termin√©e")
    return resultat

def analyser_tendances_temporelles(df):
    """Analyser les tendances temporelles"""
    logger.info("Analyse des tendances temporelles...")
    
    from pyspark.sql.functions import to_date, year, month, dayofweek
    
    resultat = df.withColumn("date_vente", to_date("timestamp")) \
        .groupBy(
            year("date_vente").alias("annee"),
            month("date_vente").alias("mois"),
            dayofweek("date_vente").alias("jour_semaine")
        ) \
        .agg(
            count("*").alias("nombre_ventes"),
            sum("total_amount").alias("revenu_total"),
            avg("total_amount").alias("panier_moyen")
        ) \
        .orderBy("annee", "mois", "jour_semaine")
    
    logger.info("‚úÖ Analyse temporelle termin√©e")
    return resultat

def sauvegarder_resultats_hdfs(resultats, nom_analyse):
    """Sauvegarder les r√©sultats sur HDFS"""
    logger.info(f"Sauvegarde des r√©sultats {nom_analyse} sur HDFS...")
    
    chemin_hdfs = f"hdfs://namenode:9000/spark-results/{nom_analyse}"
    
    resultats.write \
        .mode("overwrite") \
        .parquet(chemin_hdfs)
    
    logger.info(f"‚úÖ R√©sultats sauvegard√©s: {chemin_hdfs}")

def afficher_statistiques(df):
    """Afficher des statistiques g√©n√©rales"""
    logger.info("Calcul des statistiques g√©n√©rales...")
    
    stats = df.select(
        count("*").alias("total_ventes"),
        sum("total_amount").alias("revenu_total"),
        avg("total_amount").alias("panier_moyen"),
        countDistinct("user_id").alias("utilisateurs_uniques"),
        countDistinct("product_name").alias("produits_uniques"),
        countDistinct("category").alias("categories_uniques")
    ).collect()[0]
    
    print("\n" + "="*50)
    print("üìä STATISTIQUES G√âN√âRALES")
    print("="*50)
    print(f"Total des ventes: {stats['total_ventes']:,}")
    print(f"Revenu total: {stats['revenu_total']:,.2f} ‚Ç¨")
    print(f"Panier moyen: {stats['panier_moyen']:.2f} ‚Ç¨")
    print(f"Utilisateurs uniques: {stats['utilisateurs_uniques']:,}")
    print(f"Produits uniques: {stats['produits_uniques']:,}")
    print(f"Cat√©gories uniques: {stats['categories_uniques']:,}")
    print("="*50)

def main():
    """Fonction principale"""
    logger.info("üöÄ D√©marrage du traitement distribu√© Spark")
    
    try:
        # Cr√©er la session Spark
        spark = creer_session_spark()
        logger.info("‚úÖ Session Spark cr√©√©e")
        
        # Charger les donn√©es
        df = charger_donnees_mongodb(spark, limite=100000)
        if df is None:
            logger.error("‚ùå Impossible de charger les donn√©es")
            return
        
        # Afficher les statistiques g√©n√©rales
        afficher_statistiques(df)
        
        # Effectuer les analyses
        analyses = {
            "ventes-par-categorie": analyser_ventes_par_categorie(df),
            "top-produits": analyser_top_produits(df, 15),
            "utilisateurs-actifs": analyser_utilisateurs_actifs(df, 25),
            "tendances-temporelles": analyser_tendances_temporelles(df)
        }
        
        # Afficher et sauvegarder les r√©sultats
        for nom, resultat in analyses.items():
            print(f"\nüìà R√âSULTATS: {nom.upper()}")
            print("-" * 40)
            resultat.show(truncate=False)
            
            # Sauvegarder sur HDFS
            sauvegarder_resultats_hdfs(resultat, nom)
        
        logger.info("üéâ Traitement distribu√© termin√© avec succ√®s!")
        
    except Exception as e:
        logger.error(f"‚ùå Erreur lors du traitement: {e}")
        raise
    finally:
        if 'spark' in locals():
            spark.stop()
            logger.info("üõë Session Spark ferm√©e")

if __name__ == "__main__":
    main()
