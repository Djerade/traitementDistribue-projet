#!/usr/bin/env python3
"""
Exemple de travail Spark en mode local pour d√©montrer le traitement distribu√©
"""

from pyspark.sql import SparkSession
from pyspark.sql.functions import *
import pymongo
import logging

# Configuration du logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def creer_session_spark_local():
    """Cr√©er une session Spark en mode local"""
    return SparkSession.builder \
        .appName("TraitementLocalVentes") \
        .config("spark.driver.memory", "1g") \
        .config("spark.executor.memory", "1g") \
        .config("spark.sql.adaptive.enabled", "true") \
        .master("local[*]") \
        .getOrCreate()

def charger_donnees_mongodb(spark, limite=1000):
    """Charger les donn√©es depuis MongoDB vers Spark"""
    logger.info(f"Chargement de {limite} documents depuis MongoDB...")
    
    client = pymongo.MongoClient("mongodb://mongodb:27017")
    db = client.retail
    collection = db.sales
    
    # Charger les donn√©es
    cursor = collection.find({}, {'_id': 0}).limit(limite)
    data = list(cursor)
    
    if not data:
        logger.error("Aucune donn√©e trouv√©e dans MongoDB")
        return None
    
    # Cr√©er le DataFrame Spark
    df = spark.createDataFrame(data)
    logger.info(f"‚úÖ {df.count()} lignes charg√©es dans Spark")
    
    return df

def analyser_ventes_par_categorie(df):
    """Analyser les ventes par cat√©gorie de produits"""
    logger.info("Analyse des ventes par cat√©gorie...")
    
    resultat = df.groupBy("category") \
        .agg(
            count("*").alias("nombre_ventes"),
            sum("total_amount").alias("revenu_total"),
            avg("total_amount").alias("montant_moyen"),
            max("total_amount").alias("vente_max"),
            sum("quantity").alias("quantite_totale")
        ) \
        .orderBy(col("revenu_total").desc())
    
    logger.info("‚úÖ Analyse par cat√©gorie termin√©e")
    return resultat

def analyser_top_produits(df, top_n=10):
    """Analyser les produits les plus vendus"""
    logger.info(f"Analyse des top {top_n} produits...")
    
    resultat = df.groupBy("product_name") \
        .agg(
            count("*").alias("nombre_ventes"),
            sum("quantity").alias("quantite_totale"),
            sum("total_amount").alias("revenu_total"),
            avg("total_amount").alias("prix_moyen")
        ) \
        .orderBy(col("revenu_total").desc()) \
        .limit(top_n)
    
    logger.info("‚úÖ Analyse des top produits termin√©e")
    return resultat

def analyser_utilisateurs_actifs(df, top_n=20):
    """Analyser les utilisateurs les plus actifs"""
    logger.info(f"Analyse des {top_n} utilisateurs les plus actifs...")
    
    resultat = df.groupBy("user_id") \
        .agg(
            count("*").alias("nombre_achats"),
            sum("total_amount").alias("montant_total"),
            avg("total_amount").alias("panier_moyen"),
            countDistinct("category").alias("categories_differentes")
        ) \
        .orderBy(col("montant_total").desc()) \
        .limit(top_n)
    
    logger.info("‚úÖ Analyse des utilisateurs termin√©e")
    return resultat

def afficher_statistiques(df):
    """Afficher des statistiques g√©n√©rales"""
    logger.info("Calcul des statistiques g√©n√©rales...")
    
    stats = df.select(
        count("*").alias("total_ventes"),
        sum("total_amount").alias("revenu_total"),
        avg("total_amount").alias("panier_moyen"),
        countDistinct("user_id").alias("utilisateurs_uniques"),
        countDistinct("product_name").alias("produits_uniques"),
        countDistinct("category").alias("categories_uniques")
    ).collect()[0]
    
    print("\n" + "="*50)
    print("üìä STATISTIQUES G√âN√âRALES")
    print("="*50)
    print(f"Total des ventes: {stats['total_ventes']:,}")
    print(f"Revenu total: {stats['revenu_total']:,.2f} ‚Ç¨")
    print(f"Panier moyen: {stats['panier_moyen']:.2f} ‚Ç¨")
    print(f"Utilisateurs uniques: {stats['utilisateurs_uniques']:,}")
    print(f"Produits uniques: {stats['produits_uniques']:,}")
    print(f"Cat√©gories uniques: {stats['categories_uniques']:,}")
    print("="*50)

def main():
    """Fonction principale"""
    logger.info("üöÄ D√©marrage du traitement Spark local")
    
    try:
        # Cr√©er la session Spark
        spark = creer_session_spark_local()
        logger.info("‚úÖ Session Spark locale cr√©√©e")
        
        # Charger les donn√©es
        df = charger_donnees_mongodb(spark, limite=1000)
        if df is None:
            logger.error("‚ùå Impossible de charger les donn√©es")
            return
        
        # Afficher les statistiques g√©n√©rales
        afficher_statistiques(df)
        
        # Effectuer les analyses
        print("\nüìà R√âSULTATS: VENTES PAR CAT√âGORIE")
        print("-" * 40)
        resultat_categories = analyser_ventes_par_categorie(df)
        resultat_categories.show(truncate=False)
        
        print("\nüìà R√âSULTATS: TOP PRODUITS")
        print("-" * 40)
        resultat_produits = analyser_top_produits(df, 10)
        resultat_produits.show(truncate=False)
        
        print("\nüìà R√âSULTATS: UTILISATEURS ACTIFS")
        print("-" * 40)
        resultat_utilisateurs = analyser_utilisateurs_actifs(df, 15)
        resultat_utilisateurs.show(truncate=False)
        
        logger.info("üéâ Traitement Spark local termin√© avec succ√®s!")
        
    except Exception as e:
        logger.error(f"‚ùå Erreur lors du traitement: {e}")
        raise
    finally:
        if 'spark' in locals():
            spark.stop()
            logger.info("üõë Session Spark ferm√©e")

if __name__ == "__main__":
    main()
