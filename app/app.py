import streamlit as st
import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import pymongo
# from pyhive import hive  # Temporairement d√©sactiv√©
import os
from datetime import datetime, timedelta

# Imports Spark (optionnels - seulement si PySpark est disponible)
try:
    from pyspark.sql import SparkSession
    from pyspark.sql.functions import *
    SPARK_AVAILABLE = True
except ImportError:
    SPARK_AVAILABLE = False
    st.warning("‚ö†Ô∏è PySpark n'est pas install√©. Certaines fonctionnalit√©s Spark seront d√©sactiv√©es.")

# Configuration de la page
st.set_page_config(
    page_title="Dashboard Ventes - Traitement Distribu√©",
    page_icon="üìä",
    layout="wide"
)

# Titre principal
st.title("üìä Dashboard Ventes - Architecture Traitement Distribu√©")
st.markdown("---")

# Configuration des connexions
@st.cache_resource
def get_mongo_connection():
    """Connexion √† MongoDB"""
    try:
        # Utiliser les variables d'environnement si disponibles
        mongo_uri = os.getenv('MONGO_URI', 'mongodb://mongodb:27017/')
        client = pymongo.MongoClient(mongo_uri)
        return client
    except Exception as e:
        st.error(f"Erreur de connexion MongoDB: {e}")
        return None

@st.cache_resource
def get_hive_connection():
    """Connexion √† Hive via Spark Thrift - Temporairement d√©sactiv√©"""
    st.warning("Connexion Hive temporairement d√©sactiv√©e")
    return None

# Sidebar pour la navigation
st.sidebar.title("Navigation")
page = st.sidebar.selectbox(
    "Choisir une page",
    ["üìà Vue d'ensemble", "üîç Exploration des donn√©es", "üë• Top Utilisateurs", "üìÖ Statistiques temporelles", "‚ö° Analyses Spark"]
)

# Fonction pour charger les donn√©es depuis MongoDB
def load_mongo_data(limit=None):
    """Charger les donn√©es depuis MongoDB"""
    client = get_mongo_connection()
    if client:
        try:
            # Utiliser les variables d'environnement si disponibles
            db_name = os.getenv('MONGO_DB', 'retail')
            collection_name = os.getenv('MONGO_COLLECTION', 'sales')
            db = client[db_name]
            collection = db[collection_name]
            
            # Utiliser un curseur avec timeout et limite pour √©viter les probl√®mes de m√©moire
            cursor = collection.find({}, {'_id': 0})
            
            # Appliquer une limite si sp√©cifi√©e, sinon utiliser une limite par d√©faut
            if limit:
                cursor = cursor.limit(limit)
            else:
                # Limite par d√©faut pour √©viter les probl√®mes de m√©moire
                cursor = cursor.limit(10000)
            
            # Convertir le curseur en liste avec gestion d'erreur
            data = list(cursor)
            return pd.DataFrame(data)
        except Exception as e:
            st.error(f"Erreur lors du chargement des donn√©es MongoDB: {e}")
            return pd.DataFrame()
    return pd.DataFrame()

# Fonction pour charger les donn√©es depuis Hive
def load_hive_data(query):
    """Charger les donn√©es depuis Hive - Temporairement d√©sactiv√©"""
    st.warning("Fonction Hive temporairement d√©sactiv√©e")
    return pd.DataFrame()

# Fonction pour ex√©cuter des analyses Spark
def run_spark_analysis(analysis_type="category", limit=10000):
    """Ex√©cute des analyses Spark et retourne les r√©sultats"""
    if not SPARK_AVAILABLE:
        st.error("‚ùå PySpark n'est pas disponible. Impossible d'ex√©cuter l'analyse Spark.")
        return None
        
    try:
        # Cr√©er une session Spark en mode local
        spark = SparkSession.builder \
            .appName("StreamlitSparkAnalysis") \
            .config("spark.master", "local[*]") \
            .config("spark.driver.memory", "1g") \
            .config("spark.sql.adaptive.enabled", "true") \
            .getOrCreate()
        
        # Charger les donn√©es MongoDB
        client = get_mongo_connection()
        if not client:
            return None
            
        db_name = os.getenv('MONGO_DB', 'retail')
        collection_name = os.getenv('MONGO_COLLECTION', 'sales')
        db = client[db_name]
        collection = db[collection_name]
        
        # Charger avec limite
        cursor = collection.find({}, {'_id': 0}).limit(limit)
        data = list(cursor)
        
        if not data:
            return None
            
        # Cr√©er DataFrame Spark
        df = spark.createDataFrame(data)
        
        # Ex√©cuter l'analyse demand√©e
        if analysis_type == "category":
            result = df.groupBy("category") \
                .agg(
                    count("*").alias("nombre_ventes"),
                    sum("total_amount").alias("chiffre_affaires"),
                    avg("total_amount").alias("montant_moyen")
                ) \
                .orderBy(col("chiffre_affaires").desc())
                
        elif analysis_type == "products":
            result = df.groupBy("product_name", "category") \
                .agg(
                    count("*").alias("nombre_ventes"),
                    sum("total_amount").alias("chiffre_affaires")
                ) \
                .orderBy(col("chiffre_affaires").desc()) \
                .limit(10)
                
        elif analysis_type == "users":
            result = df.groupBy("user_id") \
                .agg(
                    count("*").alias("nombre_achats"),
                    sum("total_amount").alias("montant_total"),
                    avg("total_amount").alias("montant_moyen")
                ) \
                .orderBy(col("montant_total").desc()) \
                .limit(10)
        
        # Convertir en Pandas DataFrame
        pandas_df = result.toPandas()
        
        # Arr√™ter Spark
        spark.stop()
        
        return pandas_df
        
    except Exception as e:
        st.error(f"Erreur lors de l'analyse Spark: {e}")
        return None

# Fonction pour obtenir les statistiques MongoDB sans charger toutes les donn√©es
def get_mongo_stats():
    """Obtenir les statistiques de la base MongoDB"""
    client = get_mongo_connection()
    if client:
        try:
            db_name = os.getenv('MONGO_DB', 'retail')
            collection_name = os.getenv('MONGO_COLLECTION', 'sales')
            db = client[db_name]
            collection = db[collection_name]
            
            # Obtenir le nombre total de documents
            total_docs = collection.count_documents({})
            
            # Obtenir les statistiques d'agr√©gation
            pipeline = [
                {
                    '$group': {
                        '_id': None,
                        'total_revenue': {'$sum': '$total_amount'},
                        'avg_sale': {'$avg': '$total_amount'},
                        'unique_users': {'$addToSet': '$user_id'}
                    }
                }
            ]
            
            result = list(collection.aggregate(pipeline))
            if result:
                stats = result[0]
                return {
                    'total_docs': total_docs,
                    'total_revenue': stats['total_revenue'],
                    'avg_sale': stats['avg_sale'],
                    'unique_users': len(stats['unique_users'])
                }
        except Exception as e:
            st.error(f"Erreur lors de l'obtention des statistiques: {e}")
    
    return {
        'total_docs': 0,
        'total_revenue': 0,
        'avg_sale': 0,
        'unique_users': 0
    }

# Page Vue d'ensemble
if page == "üìà Vue d'ensemble":
    st.header("üìà Vue d'ensemble des ventes")
    
    # Obtenir les statistiques globales
    stats = get_mongo_stats()
    
    # M√©triques principales
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        st.metric("Total des ventes", f"{stats['total_docs']:,}")
    
    with col2:
        st.metric("Chiffre d'affaires", f"${stats['total_revenue']:,.2f}")
    
    with col3:
        st.metric("Vente moyenne", f"${stats['avg_sale']:.2f}")
    
    with col4:
        st.metric("Clients uniques", f"{stats['unique_users']:,}")
    
    # Charger un √©chantillon de donn√©es pour les graphiques
    df = load_mongo_data(limit=1000)
    
    if not df.empty:
        # Graphiques
        col1, col2 = st.columns(2)
        
        with col1:
            # Ventes par cat√©gorie
            category_sales = df.groupby('category')['total_amount'].sum().reset_index()
            fig_category = px.pie(
                category_sales, 
                values='total_amount', 
                names='category',
                title="R√©partition des ventes par cat√©gorie (√©chantillon)"
            )
            st.plotly_chart(fig_category, use_container_width=True)
        
        with col2:
            # Top 5 produits
            product_sales = df.groupby('product_name')['total_amount'].sum().sort_values(ascending=False).head(5)
            fig_products = px.bar(
                x=product_sales.values,
                y=product_sales.index,
                orientation='h',
                title="Top 5 des produits par chiffre d'affaires (√©chantillon)"
            )
            fig_products.update_layout(xaxis_title="Chiffre d'affaires ($)")
            st.plotly_chart(fig_products, use_container_width=True)
    else:
        st.warning("Impossible de charger les donn√©es pour les graphiques")

# Page Exploration des donn√©es
elif page == "üîç Exploration des donn√©es":
    st.header("üîç Exploration des donn√©es")
    
    # S√©lectionner la taille de l'√©chantillon
    sample_size = st.slider("Taille de l'√©chantillon", min_value=100, max_value=10000, value=1000, step=100)
    
    df = load_mongo_data(limit=sample_size)
    
    if not df.empty:
        # Filtres
        st.subheader("Filtres")
        col1, col2, col3 = st.columns(3)
        
        with col1:
            categories = ['Toutes'] + list(df['category'].unique())
            selected_category = st.selectbox("Cat√©gorie", categories)
        
        with col2:
            min_amount = st.number_input("Montant minimum ($)", min_value=0.0, value=0.0)
        
        with col3:
            max_amount = st.number_input("Montant maximum ($)", min_value=0.0, value=float(df['total_amount'].max()))
        
        # Appliquer les filtres
        filtered_df = df.copy()
        if selected_category != 'Toutes':
            filtered_df = filtered_df[filtered_df['category'] == selected_category]
        filtered_df = filtered_df[
            (filtered_df['total_amount'] >= min_amount) & 
            (filtered_df['total_amount'] <= max_amount)
        ]
        
        # Afficher les donn√©es filtr√©es
        st.subheader(f"Donn√©es filtr√©es ({len(filtered_df)} ventes sur {len(df)} √©chantillon)")
        st.dataframe(filtered_df, use_container_width=True)
        
        # Statistiques descriptives
        st.subheader("Statistiques descriptives")
        st.dataframe(filtered_df.describe(), use_container_width=True)
    else:
        st.warning("Impossible de charger les donn√©es")

# Page Top Utilisateurs
elif page == "üë• Top Utilisateurs":
    st.header("üë• Top Utilisateurs")
    
    # S√©lectionner la taille de l'√©chantillon
    sample_size = st.slider("Taille de l'√©chantillon", min_value=100, max_value=10000, value=1000, step=100)
    
    df = load_mongo_data(limit=sample_size)
    
    if not df.empty:
        # Top utilisateurs par montant d√©pens√©
        user_totals = df.groupby('user_id').agg({
            'total_amount': ['sum', 'count', 'mean']
        }).round(2)
        user_totals.columns = ['Total d√©pens√©', 'Nombre d\'achats', 'Montant moyen']
        user_totals = user_totals.sort_values('Total d√©pens√©', ascending=False)
        
        st.subheader(f"Top utilisateurs par montant total d√©pens√© (√©chantillon de {len(df)} ventes)")
        st.dataframe(user_totals, use_container_width=True)
        
        # Graphique des top utilisateurs
        top_users = user_totals.head(10)
        fig_users = px.bar(
            x=top_users.index,
            y=top_users['Total d√©pens√©'],
            title="Top 10 des utilisateurs par montant d√©pens√© (√©chantillon)"
        )
        fig_users.update_layout(xaxis_title="Utilisateur", yaxis_title="Montant total ($)")
        st.plotly_chart(fig_users, use_container_width=True)
    else:
        st.warning("Impossible de charger les donn√©es")

# Page Statistiques temporelles
elif page == "üìÖ Statistiques temporelles":
    st.header("üìÖ Statistiques temporelles")
    
    # S√©lectionner la taille de l'√©chantillon
    sample_size = st.slider("Taille de l'√©chantillon", min_value=100, max_value=10000, value=1000, step=100)
    
    df = load_mongo_data(limit=sample_size)
    
    if not df.empty:
        # Convertir la colonne date
        df['sale_date'] = pd.to_datetime(df['sale_date'])
        
        # Ventes par jour
        daily_sales = df.groupby('sale_date').agg({
            'total_amount': ['sum', 'count']
        }).round(2)
        daily_sales.columns = ['Chiffre d\'affaires', 'Nombre de ventes']
        
        # Graphique temporel
        fig_temporal = make_subplots(
            rows=2, cols=1,
            subplot_titles=('Chiffre d\'affaires quotidien', 'Nombre de ventes quotidien'),
            vertical_spacing=0.1
        )
        
        fig_temporal.add_trace(
            go.Scatter(x=daily_sales.index, y=daily_sales['Chiffre d\'affaires'], 
                      mode='lines+markers', name='CA quotidien'),
            row=1, col=1
        )
        
        fig_temporal.add_trace(
            go.Scatter(x=daily_sales.index, y=daily_sales['Nombre de ventes'], 
                      mode='lines+markers', name='Ventes quotidiennes'),
            row=2, col=1
        )
        
        fig_temporal.update_layout(height=600, title_text=f"√âvolution temporelle des ventes (√©chantillon de {len(df)} ventes)")
        st.plotly_chart(fig_temporal, use_container_width=True)
    else:
        st.warning("Impossible de charger les donn√©es")

# Page Analyses Spark
elif page == "‚ö° Analyses Spark":
    st.header("‚ö° Analyses Spark - Traitement Distribu√©")
    
    if not SPARK_AVAILABLE:
        st.error("""
        ‚ùå **PySpark n'est pas disponible**
        
        Pour utiliser les fonctionnalit√©s Spark, assurez-vous que :
        1. PySpark est install√© dans l'environnement
        2. Le service Spark est d√©marr√©
        3. Les d√©pendances sont correctement configur√©es
        """)
        
        st.info("""
        **Solution :**
        - V√©rifiez que le service `spark-processor` est en cours d'ex√©cution
        - Red√©marrez l'application avec `docker-compose restart app`
        - Consultez les logs pour plus d'informations
        """)
        
        # Afficher le statut des services
        st.subheader("üîç Statut des Services")
        col1, col2 = st.columns(2)
        
        with col1:
            st.metric("PySpark Disponible", "‚ùå Non")
        
        with col2:
            st.metric("Spark Master", "http://localhost:8085")
        
        st.stop()
    
    st.info("""
    Cette page utilise Apache Spark pour effectuer des analyses distribu√©es sur vos donn√©es.
    Spark permet de traiter de grandes quantit√©s de donn√©es en parall√®le sur plusieurs n≈ìuds.
    """)
    
    # S√©lection du type d'analyse
    analysis_type = st.selectbox(
        "Type d'analyse Spark",
        ["category", "products", "users"],
        format_func=lambda x: {
            "category": "üìä Analyse par cat√©gorie",
            "products": "üè∑Ô∏è Top produits",
            "users": "üë• Top utilisateurs"
        }[x]
    )
    
    # Taille de l'√©chantillon
    sample_size = st.slider("Taille de l'√©chantillon Spark", min_value=1000, max_value=50000, value=10000, step=1000)
    
    # Bouton pour ex√©cuter l'analyse
    if st.button("üöÄ Ex√©cuter l'analyse Spark", type="primary"):
        with st.spinner("Ex√©cution de l'analyse Spark en cours..."):
            result = run_spark_analysis(analysis_type, sample_size)
            
            if result is not None:
                st.success(f"‚úÖ Analyse Spark termin√©e ! {len(result)} r√©sultats obtenus.")
                
                # Afficher les r√©sultats
                st.subheader("üìä R√©sultats de l'analyse Spark")
                st.dataframe(result, use_container_width=True)
                
                # Graphiques selon le type d'analyse
                if analysis_type == "category":
                    fig = px.bar(
                        result, 
                        x='category', 
                        y='chiffre_affaires',
                        title="Chiffre d'affaires par cat√©gorie (Spark)",
                        color='nombre_ventes'
                    )
                    st.plotly_chart(fig, use_container_width=True)
                    
                elif analysis_type == "products":
                    fig = px.bar(
                        result, 
                        x='product_name', 
                        y='chiffre_affaires',
                        title="Top produits par chiffre d'affaires (Spark)",
                        color='category'
                    )
                    fig.update_xaxes(tickangle=45)
                    st.plotly_chart(fig, use_container_width=True)
                    
                elif analysis_type == "users":
                    fig = px.bar(
                        result, 
                        x='user_id', 
                        y='montant_total',
                        title="Top utilisateurs par montant total (Spark)",
                        color='nombre_achats'
                    )
                    st.plotly_chart(fig, use_container_width=True)
                
                # M√©triques de performance
                col1, col2, col3 = st.columns(3)
                with col1:
                    st.metric("Nombre de r√©sultats", len(result))
                with col2:
                    if 'chiffre_affaires' in result.columns:
                        total_revenue = result['chiffre_affaires'].sum()
                        st.metric("Chiffre d'affaires total", f"${total_revenue:,.2f}")
                with col3:
                    if 'montant_total' in result.columns:
                        total_amount = result['montant_total'].sum()
                        st.metric("Montant total", f"${total_amount:,.2f}")
            else:
                st.error("‚ùå Erreur lors de l'ex√©cution de l'analyse Spark")
    
    # Informations sur Spark
    st.subheader("‚ÑπÔ∏è √Ä propos d'Apache Spark")
    st.markdown("""
    **Apache Spark** est un moteur de traitement distribu√© pour le big data :
    
    - ‚ö° **Performance** : Traitement en m√©moire jusqu'√† 100x plus rapide que MapReduce
    - üîÑ **Polyvalence** : Supporte SQL, streaming, machine learning, et graph processing
    - üìä **DataFrames** : API de haut niveau pour la manipulation de donn√©es
    - üéØ **Fault tolerance** : Gestion automatique des pannes
    - üöÄ **Scalabilit√©** : Traitement distribu√© sur plusieurs n≈ìuds
    
    **Architecture actuelle :**
    - Master : namenode (Spark Master)
    - Workers : 4 n≈ìuds avec 5G RAM total
    - Stockage : HDFS + MongoDB
    """)

# Footer
st.markdown("---")
st.markdown(
    """
    <div style='text-align: center; color: gray;'>
        Architecture Traitement Distribu√© - Hadoop + Spark + Pig + MongoDB + Streamlit
    </div>
    """,
    unsafe_allow_html=True
)
